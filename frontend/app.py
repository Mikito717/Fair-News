# --- frontend/app.py ---
import streamlit as st
import requests

st.title("ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ»ãƒ•ã‚§ã‚¤ã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ¤å®šã‚¢ãƒ—ãƒª")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰è¨­å®š
st.sidebar.header("è¨­å®š")

# ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’å–å¾—
try:
    status_res = requests.get("http://localhost:8000/api/v1/status")
    if status_res.status_code == 200:
        status_data = status_res.json()
        available_backends = status_data.get("available_backends", [])
        current_backend = status_data.get("current_backend")
        available_ollama_models = status_data.get(
            "available_ollama_models", [])
        suggested_models = status_data.get("suggested_models", [])
    else:
        available_backends = []
        current_backend = None
        available_ollama_models = []
        suggested_models = []
except:
    available_backends = []
    current_backend = None
    available_ollama_models = []
    suggested_models = []

# ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é¸æŠ
if available_backends:
    backend_choice = st.sidebar.selectbox(
        "ãƒ¢ãƒ‡ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰",
        available_backends,
        index=available_backends.index(
            current_backend) if current_backend in available_backends else 0
    )

    # ãƒ¢ãƒ‡ãƒ«åé¸æŠãƒ»å…¥åŠ›
    if backend_choice == "ollama":
        if available_ollama_models:
            # åˆ©ç”¨å¯èƒ½ãªOllamaãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆã¯selectboxã§é¸æŠ
            model_name = st.sidebar.selectbox(
                "Ollamaãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
                available_ollama_models,
                help="ç¾åœ¨åˆ©ç”¨å¯èƒ½ãªOllamaãƒ¢ãƒ‡ãƒ«ä¸€è¦§"
            )
        else:
            # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆã¯æ‰‹å‹•å…¥åŠ›ã¨æ¨å¥¨ãƒ¢ãƒ‡ãƒ«è¡¨ç¤º
            st.sidebar.warning("åˆ©ç”¨å¯èƒ½ãªOllamaãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            if suggested_models:
                st.sidebar.info("æ¨å¥¨ãƒ¢ãƒ‡ãƒ«:")
                for model in suggested_models:
                    st.sidebar.code(f"ollama pull {model}")
            model_name = st.sidebar.text_input(
                "ãƒ¢ãƒ‡ãƒ«åã‚’å…¥åŠ›",
                value="llama3.2",
                help="ã¾ãšãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„: ollama pull <model_name>"
            )
    else:
        # Transformersã®å ´åˆã¯æ‰‹å‹•å…¥åŠ›
        model_name = st.sidebar.text_input(
            "Transformersãƒ¢ãƒ‡ãƒ«å",
            value="rinna/japanese-gpt-neox-3.6b-instruction-sft",
            help="Hugging Faceã®ãƒ¢ãƒ‡ãƒ«åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
        )

    # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰åˆ‡ã‚Šæ›¿ãˆãƒœã‚¿ãƒ³
    if st.sidebar.button("ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆ"):
        try:
            switch_res = requests.post(
                "http://localhost:8000/api/v1/switch-backend",
                json={"backend": backend_choice, "model_name": model_name}
            )
            if switch_res.status_code == 200:
                st.sidebar.success(f"{backend_choice} ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
                st.experimental_rerun()
            else:
                st.sidebar.error("ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®åˆ‡ã‚Šæ›¿ãˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        except Exception as e:
            st.sidebar.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
else:
    st.sidebar.warning("åˆ©ç”¨å¯èƒ½ãªãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
    backend_choice = None
    model_name = None

# ç¾åœ¨ã®çŠ¶æ…‹è¡¨ç¤º
if current_backend:
    st.sidebar.info(f"ç¾åœ¨ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰: {current_backend}")

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã®è¡¨ç¤º
if available_ollama_models:
    with st.sidebar.expander("åˆ©ç”¨å¯èƒ½ãªOllamaãƒ¢ãƒ‡ãƒ«"):
        for model in available_ollama_models:
            st.write(f"â€¢ {model}")

article = st.text_area("ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", height=200)

if st.button("åˆ¤å®šã™ã‚‹"):
    if not article.strip():
        st.error("è¨˜äº‹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        with st.status("åˆ¤å®šã‚’é–‹å§‹ã—ã¾ã™...", expanded=True) as status:
            request_data = {"article": article}
            if backend_choice and model_name:
                request_data["backend"] = backend_choice
                request_data["model_name"] = model_name

            res = requests.post(
                "http://localhost:8000/api/v1/judge",
                json=request_data
            )
            if res.status_code == 200:
                data = res.json()
                status.update(label="åˆ¤å®šå®Œäº†", state="complete")

                # ãƒ¡ã‚¿æƒ…å ±è¡¨ç¤º
                meta = data.get('meta', {})
                st.markdown("## åˆ¤å®šçµæœ")
                st.caption(
                    f"å®Ÿè¡Œæ™‚é–“: {meta.get('execution_time', 'N/A')} | ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰: {meta.get('backend', 'N/A')} | ãƒ¢ãƒ‡ãƒ«: {meta.get('model_name', 'N/A')}")

                # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥çµæœè¡¨ç¤º
                for agent in ["liberal", "conservative", "neutral"]:
                    agent_names = {
                        "liberal": "ğŸŒ¿ ãƒªãƒ™ãƒ©ãƒ«è¦–ç‚¹",
                        "conservative": "ğŸ›ï¸ ä¿å®ˆçš„è¦–ç‚¹",
                        "neutral": "âš–ï¸ ä¸­ç«‹çš„è¦–ç‚¹"
                    }

                    st.subheader(agent_names[agent])
                    bias_score = data['results'][agent]['bias_score']
                    st.markdown(
                        f"**ãƒã‚¤ã‚¢ã‚¹ã‚¹ã‚³ã‚¢:** {bias_score:.2f} ({'ä¸­ç«‹' if bias_score < 0.3 else 'è»½åº¦ã®åã‚Š' if bias_score < 0.7 else 'å¼·ã„åã‚Š'})")
                    st.markdown("**åˆ†æ:**")
                    st.write(data['results'][agent]['summary'])
                    st.divider()
            else:
                status.update(label="APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ", state="error")
                st.error(f"APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {res.status_code}")
                try:
                    error_detail = res.json()
                    st.error(
                        f"è©³ç´°: {error_detail.get('detail', 'Unknown error')}")
                except:
                    pass
